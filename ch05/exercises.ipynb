{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# Exercise 5.1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T00:21:50.168827Z",
     "start_time": "2025-04-24T00:21:49.964728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def softmax_with_temperature(logits, temperature=1.0):\n",
    "    \"\"\"Compute softmax with temperature scaling\"\"\"\n",
    "    scaled_logits = logits / temperature\n",
    "    exp_logits = np.exp(scaled_logits - np.max(scaled_logits))  # for numerical stability\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def print_sampled_tokens(probs, words, num_samples=1000):\n",
    "    \"\"\"Sample tokens according to probabilities and print frequencies\"\"\"\n",
    "    samples = np.random.choice(words, size=num_samples, p=probs)\n",
    "    freq = Counter(samples)\n",
    "    for word in words:\n",
    "        print(f\"{word}: {freq[word]/num_samples:.2%}\")\n",
    "    return freq\n",
    "\n",
    "# Original probabilities (approximated from the graph)\n",
    "original_probs = np.array([0.1, 0.05, 0.15, 0.05, 0.1, 0.5, 0.05, 0.0])\n",
    "words = ['closer', 'every effort', 'forward', 'inches', 'moves', 'pizza', 'toward', 'you']\n",
    "\n",
    "# Temperature values to test\n",
    "temperatures = [0.1, 1.0, 5.0]\n",
    "num_samples = 10000\n",
    "\n",
    "print(\"Sampling frequencies:\")\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTemperature = {temp}\")\n",
    "    scaled_probs = softmax_with_temperature(original_probs, temperature=temp)\n",
    "    freq = print_sampled_tokens(scaled_probs, words, num_samples)\n",
    "    pizza_freq = freq['pizza'] / num_samples\n",
    "    print(f\"\\nPizza sampling frequency at T={temp}: {pizza_freq:.2%}\")\n",
    "\n",
    "    # More accurate way to get pizza frequency (without sampling)\n",
    "    accurate_pizza_prob = scaled_probs[words.index('pizza')]\n",
    "    print(f\"Exact pizza probability at T={temp}: {accurate_pizza_prob:.2%}\")"
   ],
   "id": "deabbce619421155",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling frequencies:\n",
      "\n",
      "Temperature = 0.1\n",
      "closer: 1.73%\n",
      "every effort: 0.94%\n",
      "forward: 2.32%\n",
      "inches: 1.12%\n",
      "moves: 1.83%\n",
      "pizza: 90.33%\n",
      "toward: 1.04%\n",
      "you: 0.69%\n",
      "\n",
      "Pizza sampling frequency at T=0.1: 90.33%\n",
      "Exact pizza probability at T=0.1: 90.34%\n",
      "\n",
      "Temperature = 1.0\n",
      "closer: 12.21%\n",
      "every effort: 11.48%\n",
      "forward: 13.24%\n",
      "inches: 11.12%\n",
      "moves: 11.90%\n",
      "pizza: 17.82%\n",
      "toward: 11.03%\n",
      "you: 11.20%\n",
      "\n",
      "Pizza sampling frequency at T=1.0: 17.82%\n",
      "Exact pizza probability at T=1.0: 17.97%\n",
      "\n",
      "Temperature = 5.0\n",
      "closer: 12.11%\n",
      "every effort: 12.39%\n",
      "forward: 12.43%\n",
      "inches: 12.39%\n",
      "moves: 12.16%\n",
      "pizza: 13.71%\n",
      "toward: 12.30%\n",
      "you: 12.51%\n",
      "\n",
      "Pizza sampling frequency at T=5.0: 13.71%\n",
      "Exact pizza probability at T=5.0: 13.47%\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Exercise 5.2",
   "id": "afd83dda61ad6d61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T00:21:53.174692Z",
     "start_time": "2025-04-24T00:21:53.104788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def softmax_with_temperature(logits, temperature=1.0):\n",
    "    scaled_logits = logits / temperature\n",
    "    exp_logits = np.exp(scaled_logits - np.max(scaled_logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def top_k_sampling(probs, k):\n",
    "    top_k_indices = np.argsort(probs)[-k:]\n",
    "    top_k_probs = probs[top_k_indices]\n",
    "    return top_k_probs / top_k_probs.sum(), top_k_indices\n",
    "\n",
    "def sample_with_settings(logits, words, temperature=1.0, top_k=None, num_samples=1000):\n",
    "    probs = softmax_with_temperature(logits, temperature)\n",
    "\n",
    "    if top_k is not None:\n",
    "        top_k_probs, top_k_indices = top_k_sampling(probs, top_k)\n",
    "        sampled_words = [words[i] for i in top_k_indices]\n",
    "        samples = np.random.choice(sampled_words, size=num_samples, p=top_k_probs)\n",
    "    else:\n",
    "        samples = np.random.choice(words, size=num_samples, p=probs)\n",
    "\n",
    "    freq = Counter(samples)\n",
    "    print(f\"\\nTemperature: {temperature}, Top-k: {top_k}\")\n",
    "    for word in words:\n",
    "        if word in freq:\n",
    "            print(f\"{word}: {freq[word]/num_samples:.2%}\")\n",
    "        else:\n",
    "            print(f\"{word}: 0.00%\")\n",
    "\n",
    "    return freq\n",
    "\n",
    "# Example word distribution (logits)\n",
    "logits = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0])\n",
    "words = ['closer', 'every effort', 'forward', 'inches', 'moves', 'pizza', 'toward', 'you']\n",
    "\n",
    "# Experiment with different settings\n",
    "settings = [\n",
    "    {'temperature': 0.1, 'top_k': None},\n",
    "    {'temperature': 0.1, 'top_k': 3},\n",
    "    {'temperature': 1.0, 'top_k': None},\n",
    "    {'temperature': 1.0, 'top_k': 5},\n",
    "    {'temperature': 5.0, 'top_k': None},\n",
    "    {'temperature': 5.0, 'top_k': 8}\n",
    "]\n",
    "\n",
    "for setting in settings:\n",
    "    sample_with_settings(logits, words,\n",
    "                        temperature=setting['temperature'],\n",
    "                        top_k=setting['top_k'],\n",
    "                        num_samples=10000)"
   ],
   "id": "18210b1e5c8302ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temperature: 0.1, Top-k: None\n",
      "closer: 0.00%\n",
      "every effort: 0.00%\n",
      "forward: 0.00%\n",
      "inches: 0.00%\n",
      "moves: 0.00%\n",
      "pizza: 0.00%\n",
      "toward: 0.00%\n",
      "you: 100.00%\n",
      "\n",
      "Temperature: 0.1, Top-k: 3\n",
      "closer: 0.00%\n",
      "every effort: 0.00%\n",
      "forward: 0.00%\n",
      "inches: 0.00%\n",
      "moves: 0.00%\n",
      "pizza: 0.00%\n",
      "toward: 0.00%\n",
      "you: 100.00%\n",
      "\n",
      "Temperature: 1.0, Top-k: None\n",
      "closer: 0.07%\n",
      "every effort: 0.26%\n",
      "forward: 0.52%\n",
      "inches: 1.06%\n",
      "moves: 3.29%\n",
      "pizza: 9.19%\n",
      "toward: 23.06%\n",
      "you: 62.55%\n",
      "\n",
      "Temperature: 1.0, Top-k: 5\n",
      "closer: 0.00%\n",
      "every effort: 0.00%\n",
      "forward: 0.00%\n",
      "inches: 1.24%\n",
      "moves: 3.45%\n",
      "pizza: 8.35%\n",
      "toward: 23.33%\n",
      "you: 63.63%\n",
      "\n",
      "Temperature: 5.0, Top-k: None\n",
      "closer: 5.75%\n",
      "every effort: 6.84%\n",
      "forward: 8.43%\n",
      "inches: 9.83%\n",
      "moves: 12.63%\n",
      "pizza: 15.65%\n",
      "toward: 18.43%\n",
      "you: 22.44%\n",
      "\n",
      "Temperature: 5.0, Top-k: 8\n",
      "closer: 6.11%\n",
      "every effort: 6.45%\n",
      "forward: 8.53%\n",
      "inches: 10.56%\n",
      "moves: 12.35%\n",
      "pizza: 14.54%\n",
      "toward: 18.75%\n",
      "you: 22.71%\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Preferred Applications:\n",
    "# Technical Writing: Code generation, technical documentation where precision is crucial\n",
    "# Factual Responses: QA systems where accuracy is paramount\n",
    "# Medical/Legal Applications: Where incorrect information could have serious consequences\n",
    "# Summarization: Extracting key information without creative additions\n",
    "# Translation: Maintaining faithfulness to the original text"
   ],
   "id": "1486fe0cf6a301f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Exercise 5.3",
   "id": "3fa7205f85b22764"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T00:22:03.658099Z",
     "start_time": "2025-04-24T00:22:03.652758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_deterministic(prompt, model, max_length=50):\n",
    "    \"\"\"\n",
    "    Fully deterministic generation that will always produce the same output\n",
    "    for the same input prompt and model state.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt: Input text to start generation\n",
    "    - model: The language model to use for generation\n",
    "    - max_length: Maximum number of tokens to generate\n",
    "\n",
    "    Returns:\n",
    "    - Generated text (always the same for same inputs)\n",
    "    \"\"\"\n",
    "    # Set all possible randomization controls\n",
    "    generation_config = {\n",
    "        'do_sample': False,       # Disable sampling\n",
    "        'temperature': 0,        # Effectively disables temperature\n",
    "        'top_k': 1,              # Only consider top 1 token\n",
    "        'top_p': 0,              # Disable nucleus sampling\n",
    "        'num_beams': 1,          # No beam search\n",
    "        'repetition_penalty': 1, # No repetition adjustment\n",
    "        'seed': 42,              # Fixed random seed\n",
    "    }\n",
    "\n",
    "    # Generate text with deterministic settings\n",
    "    output = model.generate(\n",
    "        prompt,\n",
    "        max_length=max_length,\n",
    "        **generation_config\n",
    "    )\n",
    "\n",
    "    return output"
   ],
   "id": "3df9e581bfa2f6c3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Exercise 5.4",
   "id": "3a9e4753b913a2b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T00:22:12.108924Z",
     "start_time": "2025-04-24T00:22:05.726970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Tiny Model Configuration\n",
    "class MiniGPTConfig:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 1000  # Reduced vocabulary\n",
    "        self.n_layer = 2        # Only 2 layers\n",
    "        self.n_head = 2         # 2 attention heads\n",
    "        self.n_embd = 64        # Tiny embeddings\n",
    "        self.block_size = 32    # Short sequences\n",
    "\n",
    "# 2. Tiny Model Definition\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(self.tok_emb(x))\n",
    "\n",
    "# 3. Training Setup\n",
    "def main():\n",
    "    # Configuration\n",
    "    config = MiniGPTConfig()\n",
    "    device = torch.device(\"cpu\")  # Force CPU to avoid GPU memory issues\n",
    "\n",
    "    # Initialize tiny model\n",
    "    model = MiniGPT(config).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Tiny batch of data\n",
    "    inputs = torch.randint(0, config.vocab_size, (2, 16)).to(device)  # 2 samples, 16 tokens\n",
    "    targets = torch.randint(0, config.vocab_size, (2, 16)).to(device)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for step in range(3):  # Just 3 steps for demonstration\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = nn.functional.cross_entropy(outputs.view(-1, outputs.size(-1)),\n",
    "                                         targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Step {step+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(\"Completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting minimal training example...\")\n",
    "    main()"
   ],
   "id": "37b4cc9463ddf060",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting minimal training example...\n",
      "Step 1, Loss: 7.1478\n",
      "Step 2, Loss: 7.0858\n",
      "Step 3, Loss: 7.0240\n",
      "Completed successfully!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Exercise 5.5",
   "id": "adb139fd6506316b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T00:22:34.022376Z",
     "start_time": "2025-04-24T00:22:14.418542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device):\n",
    "    \"\"\"Calculate loss on dataset using data loader\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            masks = batch['attention_mask'].to(device)\n",
    "            labels = inputs.clone()  # For language modeling, labels are same as inputs\n",
    "\n",
    "            outputs = model(inputs, attention_mask=masks, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            batch_size = inputs.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "    return total_loss / total_samples\n",
    "\n",
    "# Initialize GPT model (124M parameter version)\n",
    "model_name = \"gpt2\"  # This is the 124M parameter model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set pad token if not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Prepare dataset (replace with actual \"The Verdict\" dataset)\n",
    "class VerdictDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(\n",
    "            texts,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx]\n",
    "        }\n",
    "\n",
    "# Sample data - REPLACE WITH ACTUAL \"The Verdict\" DATASET\n",
    "train_texts = [\"The verdict was guilty...\", \"In the case of State v. Smith...\"]\n",
    "val_texts = [\"The judge ruled in favor...\", \"After considering the evidence...\"]\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = VerdictDataset(train_texts, tokenizer)\n",
    "val_dataset = VerdictDataset(val_texts, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "# Calculate losses\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "train_loss = calc_loss_loader(train_loader, model, device)\n",
    "val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(f\"Training loss: {train_loss}\")\n",
    "print(f\"Validation loss: {val_loss}\")"
   ],
   "id": "4794537ff423d06",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kamal\\PycharmProjects\\LLMs-from-scratch\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.598292350769043\n",
      "Validation loss: 8.96030044555664\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Exercise 5.6",
   "id": "7db52f2adafa3e1b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-24T00:22:36.334675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from time import time\n",
    "\n",
    "def load_model_efficiently(model_size, device):\n",
    "    \"\"\"Load model with optimal settings for given hardware\"\"\"\n",
    "    model_name = \"gpt2\" if model_size == \"124M\" else \"gpt2-xl\"\n",
    "\n",
    "    # Precision control based on device\n",
    "    torch_dtype = torch.float16 if \"cuda\" in device else torch.float32\n",
    "\n",
    "    return GPT2LMHeadModel.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device_map=\"auto\" if \"cuda\" in device else None,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "def generate_efficient_sample(model, tokenizer, prompt, device):\n",
    "    \"\"\"Optimized generation with hardware-aware settings\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.inference_mode():  # Faster alternative to torch.no_grad()\n",
    "        return model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,  # More efficient than max_length\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "def benchmark_model(model_size, prompts, device):\n",
    "    \"\"\"Complete benchmarking routine\"\"\"\n",
    "    print(f\"\\n=== Benchmarking {model_size} model ===\")\n",
    "\n",
    "    # Load with progress monitoring\n",
    "    start = time()\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\" if model_size == \"124M\" else \"gpt2-xl\")\n",
    "    model = load_model_efficiently(model_size, device)\n",
    "    load_time = time() - start\n",
    "    print(f\"Model loaded in {load_time:.2f}s | Device: {device}\")\n",
    "\n",
    "    # Generation benchmarks\n",
    "    for prompt in prompts:\n",
    "        start_gen = time()\n",
    "        outputs = generate_efficient_sample(model, tokenizer, prompt, device)\n",
    "        gen_time = time() - start_gen\n",
    "        print(f\"\\nPrompt: '{prompt}'\\nGenerated: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")\n",
    "        print(f\"Generated in {gen_time:.2f}s ({len(outputs[0])} tokens)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    test_prompts = [\n",
    "        \"The verdict in the case was\",\n",
    "        \"In a surprising legal development,\"\n",
    "    ]\n",
    "\n",
    "    # Run benchmarks\n",
    "    for model_size in [\"124M\", \"1558M\"]:\n",
    "        try:\n",
    "            benchmark_model(model_size, test_prompts, device)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to benchmark {model_size}: {str(e)}\")\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                print(\"Try reducing model size or using a GPU with more memory\")"
   ],
   "id": "92cf7372c718a676",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Benchmarking 124M model ===\n",
      "Model loaded in 1.92s | Device: cpu\n",
      "\n",
      "Prompt: 'The verdict in the case was'\n",
      "Generated: The verdict in the case was unanimous.\n",
      "\n",
      "\"In the end, the trial court found that the defendant acted in good faith when he and his friends made a false statement to a police officer, which was an act of maliciousness,\" the Supreme Court said.\n",
      "\n",
      "The\n",
      "Generated in 3.19s (56 tokens)\n",
      "\n",
      "Prompt: 'In a surprising legal development,'\n",
      "Generated: In a surprising legal development, the court has allowed the government to use a computer-generated image of a man who has been convicted of raping a 12-year-old girl.\n",
      "\n",
      "The victim was discovered in the middle of the night last November with her head in her hands\n",
      "Generated in 2.13s (56 tokens)\n",
      "\n",
      "=== Benchmarking 1558M model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kamal\\PycharmProjects\\LLMs-from-scratch\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kamal\\.cache\\huggingface\\hub\\models--gpt2-xl. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
